//! Anthropic API Handler å‡½æ•°

use std::convert::Infallible;

use crate::kiro::model::events::Event;
use crate::kiro::model::requests::kiro::KiroRequest;
use crate::kiro::parser::decoder::EventStreamDecoder;
use crate::kiro::provider::StreamResponse;
use crate::kiro::token_manager::ConnectionGuard;
use crate::token;
use axum::{
    Json as JsonExtractor,
    body::Body,
    extract::State,
    http::{StatusCode, header},
    response::{IntoResponse, Json, Response},
};
use bytes::Bytes;
use futures::{Stream, StreamExt, stream};
use serde_json::json;
use std::time::Duration;
use tokio::time::interval;
use uuid::Uuid;

use super::converter::{ConversionError, convert_request};
use super::middleware::AppState;
use super::stream::{SseEvent, StreamContext};
use super::types::{
    CountTokensRequest, CountTokensResponse, ErrorResponse, MessagesRequest, Model, ModelsResponse,
};
use super::websearch;

/// GET /v1/models
///
/// è¿”å›å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
pub async fn get_models() -> impl IntoResponse {
    tracing::info!("Received GET /v1/models request");

    let models = vec![
        Model {
            id: "claude-sonnet-4-5-20250929".to_string(),
            object: "model".to_string(),
            created: 1727568000,
            owned_by: "anthropic".to_string(),
            display_name: "Claude Sonnet 4.5".to_string(),
            model_type: "chat".to_string(),
            max_tokens: 32000,
        },
        Model {
            id: "claude-opus-4-5-20251101".to_string(),
            object: "model".to_string(),
            created: 1730419200,
            owned_by: "anthropic".to_string(),
            display_name: "Claude Opus 4.5".to_string(),
            model_type: "chat".to_string(),
            max_tokens: 32000,
        },
        Model {
            id: "claude-haiku-4-5-20251001".to_string(),
            object: "model".to_string(),
            created: 1727740800,
            owned_by: "anthropic".to_string(),
            display_name: "Claude Haiku 4.5".to_string(),
            model_type: "chat".to_string(),
            max_tokens: 32000,
        },
    ];

    Json(ModelsResponse {
        object: "list".to_string(),
        data: models,
    })
}

/// POST /v1/messages
///
/// åˆ›å»ºæ¶ˆæ¯ï¼ˆå¯¹è¯ï¼‰
pub async fn post_messages(
    State(state): State<AppState>,
    JsonExtractor(payload): JsonExtractor<MessagesRequest>,
) -> Response {
    tracing::info!(
        model = %payload.model,
        max_tokens = %payload.max_tokens,
        stream = %payload.stream,
        message_count = %payload.messages.len(),
        "Received POST /v1/messages request"
    );
    // æ£€æŸ¥ KiroProvider æ˜¯å¦å¯ç”¨
    let provider = match &state.kiro_provider {
        Some(p) => p.clone(),
        None => {
            tracing::error!("KiroProvider æœªé…ç½®");
            return (
                StatusCode::SERVICE_UNAVAILABLE,
                Json(ErrorResponse::new(
                    "service_unavailable",
                    "Kiro API provider not configured",
                )),
            )
                .into_response();
        }
    };

    // æ£€æŸ¥æ˜¯å¦ä¸º WebSearch è¯·æ±‚
    if websearch::has_web_search_tool(&payload) {
        tracing::info!("æ£€æµ‹åˆ° WebSearch å·¥å…·ï¼Œè·¯ç”±åˆ° WebSearch å¤„ç†");

        // ä¼°ç®—è¾“å…¥ tokens
        let input_tokens = token::count_all_tokens(
            payload.model.clone(),
            payload.system.clone(),
            payload.messages.clone(),
            payload.tools.clone(),
        ) as i32;

        return websearch::handle_websearch_request(provider, &payload, input_tokens).await;
    }

    // è½¬æ¢è¯·æ±‚
    let conversion_result = match convert_request(&payload) {
        Ok(result) => result,
        Err(e) => {
            let (error_type, message) = match &e {
                ConversionError::UnsupportedModel(model) => {
                    ("invalid_request_error", format!("æ¨¡å‹ä¸æ”¯æŒ: {}", model))
                }
                ConversionError::EmptyMessages => {
                    ("invalid_request_error", "æ¶ˆæ¯åˆ—è¡¨ä¸ºç©º".to_string())
                }
            };
            tracing::warn!("è¯·æ±‚è½¬æ¢å¤±è´¥: {}", e);
            return (
                StatusCode::BAD_REQUEST,
                Json(ErrorResponse::new(error_type, message)),
            )
                .into_response();
        }
    };

    // æ„å»º Kiro è¯·æ±‚
    let kiro_request = KiroRequest {
        conversation_state: conversion_result.conversation_state,
        profile_arn: state.profile_arn.clone(),
    };

    let request_body = match serde_json::to_string(&kiro_request) {
        Ok(body) => {
            let body_size = body.len();
            let body_size_kb = body_size as f64 / 1024.0;

            // åˆ†æè¯·æ±‚ä½“å†…å®¹ç»„æˆ
            let message_count = payload.messages.len();
            let system_size = payload.system.as_ref()
                .map(|s| serde_json::to_string(s).unwrap_or_default().len())
                .unwrap_or(0);
            let tools_size = payload.tools.as_ref()
                .map(|t| serde_json::to_string(t).unwrap_or_default().len())
                .unwrap_or(0);

            // è®¡ç®—æ¶ˆæ¯å†…å®¹çš„å¹³å‡å¤§å°
            let avg_message_size = if message_count > 0 {
                body_size / message_count
            } else {
                0
            };

            tracing::info!(
                "ğŸ“Š è¯·æ±‚ä½“åˆ†æ - æ€»å¤§å°: {} bytes ({:.2} KB), æ¶ˆæ¯æ•°: {}, å¹³å‡æ¯æ¡: {} bytes, system: {} bytes, tools: {} bytes",
                body_size,
                body_size_kb,
                message_count,
                avg_message_size,
                system_size,
                tools_size
            );

            // è­¦å‘Šé˜ˆå€¼æ£€æŸ¥
            if body_size > 2_000_000 {
                tracing::error!(
                    "âŒ è¯·æ±‚ä½“è¿‡å¤§: {:.2} MBï¼Œè¶…è¿‡ Kiro API å¯èƒ½çš„é™åˆ¶ï¼ˆ~2MBï¼‰",
                    body_size as f64 / 1024.0 / 1024.0
                );

                // åˆ†ææ¶ˆæ¯å¤§å°åˆ†å¸ƒ
                if message_count > 0 {
                    let mut message_sizes: Vec<(usize, usize)> = payload.messages.iter()
                        .enumerate()
                        .map(|(idx, msg)| {
                            let size = serde_json::to_string(msg).unwrap_or_default().len();
                            (idx, size)
                        })
                        .collect();

                    // æŒ‰å¤§å°æ’åºï¼Œæ‰¾å‡ºæœ€å¤§çš„å‡ æ¡æ¶ˆæ¯
                    message_sizes.sort_by(|a, b| b.1.cmp(&a.1));

                    tracing::error!("ğŸ“‹ æœ€å¤§çš„ 5 æ¡æ¶ˆæ¯:");
                    for (idx, size) in message_sizes.iter().take(5) {
                        tracing::error!("  æ¶ˆæ¯ #{}: {:.2} KB", idx + 1, *size as f64 / 1024.0);
                    }
                }
            } else if body_size > 1_500_000 {
                tracing::warn!(
                    "âš ï¸  è¯·æ±‚ä½“æ¥è¿‘é™åˆ¶: {:.2} MBï¼Œå»ºè®®ä½¿ç”¨ /compact å‹ç¼©ä¸Šä¸‹æ–‡",
                    body_size as f64 / 1024.0 / 1024.0
                );
            } else if body_size > 1_000_000 {
                tracing::warn!(
                    "âš ï¸  è¯·æ±‚ä½“è¾ƒå¤§: {:.2} MB",
                    body_size as f64 / 1024.0 / 1024.0
                );
            }

            body
        }
        Err(e) => {
            tracing::error!("åºåˆ—åŒ–è¯·æ±‚å¤±è´¥: {}", e);
            return (
                StatusCode::INTERNAL_SERVER_ERROR,
                Json(ErrorResponse::new(
                    "internal_error",
                    format!("åºåˆ—åŒ–è¯·æ±‚å¤±è´¥: {}", e),
                )),
            )
                .into_response();
        }
    };

    tracing::debug!("Kiro request body: {}", request_body);

    // ä¼°ç®—è¾“å…¥ tokens
    let input_tokens = token::count_all_tokens(
        payload.model.clone(),
        payload.system.clone(),
        payload.messages.clone(),
        payload.tools.clone(),
    ) as i32;

    tracing::info!(
        "Token è®¡æ•° - æ¶ˆæ¯æ•°: {}, è¾“å…¥ tokens: {}",
        payload.messages.len(),
        input_tokens
    );

    // è·å–æ¨¡å‹çš„context windowå¤§å°
    let context_window_size = super::model_config::get_context_window_size(&payload.model);

    // æå‰æ£€æŸ¥ï¼šinput_tokens + max_tokens æ˜¯å¦è¶…è¿‡context window
    let total_tokens = input_tokens + payload.max_tokens;
    if total_tokens > context_window_size {
        tracing::warn!(
            "è¯·æ±‚è¢«æ‹¦æˆª: input_tokens({}) + max_tokens({}) = {} > context_window({})",
            input_tokens,
            payload.max_tokens,
            total_tokens,
            context_window_size
        );

        return (
            StatusCode::BAD_REQUEST,
            Json(ErrorResponse::new(
                "invalid_request_error",
                format!(
                    "input length and max_tokens exceed context limit: {} + {} > {}, decrease input length or max_tokens and try again. Suggestion: 1) Use /compact command to reduce context 2) Reduce conversation history 3) Decrease max_tokens parameter",
                    input_tokens,
                    payload.max_tokens,
                    context_window_size
                ),
            )),
        )
            .into_response();
    }

    // æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†thinking
    let thinking_enabled = payload
        .thinking
        .as_ref()
        .map(|t| t.thinking_type == "enabled")
        .unwrap_or(false);

    if payload.stream {
        // æµå¼å“åº”
        handle_stream_request(
            provider,
            &request_body,
            &payload.model,
            input_tokens,
            thinking_enabled,
        )
        .await
    } else {
        // éæµå¼å“åº”
        handle_non_stream_request(provider, &request_body, &payload.model, input_tokens).await
    }
}

/// æ ¹æ®ä¸Šæ¸¸é”™è¯¯ä¿¡æ¯åˆ¤æ–­åº”è¿”å›çš„çŠ¶æ€ç 
fn determine_error_status(error_msg: &str) -> (StatusCode, &'static str) {
    if error_msg.contains("400 Bad Request") {
        (StatusCode::BAD_REQUEST, "invalid_request_error")
    } else if error_msg.contains("429") {
        (StatusCode::TOO_MANY_REQUESTS, "rate_limit_error")
    } else if error_msg.contains("401") || error_msg.contains("403") {
        (StatusCode::UNAUTHORIZED, "authentication_error")
    } else {
        (StatusCode::BAD_GATEWAY, "api_error")
    }
}

/// æ£€æŸ¥é”™è¯¯ä¿¡æ¯æ˜¯å¦ä¸ºtokenè¶…é™é”™è¯¯
fn is_token_limit_error(error_msg: &str) -> bool {
    error_msg.contains("Input is too long")
        || error_msg.contains("too long")
        || error_msg.contains("exceeds")
        || error_msg.contains("CONTENT_LENGTH_EXCEEDS_THRESHOLD")
        || error_msg.contains("context limit")
}

/// ç”Ÿæˆå‹å¥½çš„tokenè¶…é™é”™è¯¯ä¿¡æ¯
fn create_token_limit_error(input_tokens: i32, max_tokens: i32, context_window: i32) -> ErrorResponse {
    ErrorResponse::new(
        "invalid_request_error",
        format!(
            "Prompt is too long (server-side context limit reached). Input tokens: {}, Max tokens: {}, Context window: {}. Suggestion: 1) Use /compact command to reduce context 2) Reduce conversation history 3) Decrease max_tokens parameter",
            input_tokens,
            max_tokens,
            context_window
        ),
    )
}

/// å¤„ç†æµå¼è¯·æ±‚
async fn handle_stream_request(
    provider: std::sync::Arc<crate::kiro::provider::KiroProvider>,
    request_body: &str,
    model: &str,
    input_tokens: i32,
    thinking_enabled: bool,
) -> Response {
    tracing::info!(
        "å¼€å§‹å¤„ç†æµå¼è¯·æ±‚ - model: {}, input_tokens: {}, thinking: {}",
        model,
        input_tokens,
        thinking_enabled
    );

    // è°ƒç”¨ Kiro APIï¼ˆæ”¯æŒå¤šå‡­æ®æ•…éšœè½¬ç§»ï¼‰
    let stream_response = match provider.call_api_stream(request_body).await {
        Ok(resp) => resp,
        Err(e) => {
            let error_msg = e.to_string();
            tracing::error!("Kiro API è°ƒç”¨å¤±è´¥: {}", error_msg);

            // æ£€æŸ¥æ˜¯å¦ä¸ºtokenè¶…é™é”™è¯¯
            if is_token_limit_error(&error_msg) {
                let context_window = super::model_config::get_context_window_size(model);
                // ä»request_bodyè§£æmax_tokensï¼ˆç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼‰
                let max_tokens = 8192; // é»˜è®¤å€¼ï¼Œå®é™…åº”è¯¥ä»payloadè·å–
                return (
                    StatusCode::BAD_REQUEST,
                    Json(create_token_limit_error(input_tokens, max_tokens, context_window)),
                )
                    .into_response();
            }

            let (status, error_type) = determine_error_status(&error_msg);
            return (
                status,
                Json(ErrorResponse::new(
                    error_type,
                    format!("ä¸Šæ¸¸ API è°ƒç”¨å¤±è´¥: {}", error_msg),
                )),
            )
                .into_response();
        }
    };

    // è§£æ„ StreamResponseï¼Œè·å– response å’Œ guard
    let StreamResponse { response, guard } = stream_response;

    // åˆ›å»ºæµå¤„ç†ä¸Šä¸‹æ–‡
    let mut ctx = StreamContext::new_with_thinking(model, input_tokens, thinking_enabled);

    // ç”Ÿæˆåˆå§‹äº‹ä»¶
    let initial_events = ctx.generate_initial_events();

    // åˆ›å»º SSE æµï¼Œä¼ å…¥ guard ä»¥ä¿æŒå…¶ç”Ÿå‘½å‘¨æœŸ
    let stream = create_sse_stream(response, ctx, initial_events, guard);

    // è¿”å› SSE å“åº”
    Response::builder()
        .status(StatusCode::OK)
        .header(header::CONTENT_TYPE, "text/event-stream")
        .header(header::CACHE_CONTROL, "no-cache")
        .header(header::CONNECTION, "keep-alive")
        .body(Body::from_stream(stream))
        .unwrap()
}

/// Ping äº‹ä»¶é—´éš”ï¼ˆ25ç§’ï¼‰
const PING_INTERVAL_SECS: u64 = 25;

/// åˆ›å»º ping äº‹ä»¶çš„ SSE å­—ç¬¦ä¸²
fn create_ping_sse() -> Bytes {
    Bytes::from("event: ping\ndata: {\"type\": \"ping\"}\n\n")
}

/// åˆ›å»º SSE äº‹ä»¶æµ
///
/// guard å‚æ•°ç”¨äºä¿æŒ ConnectionGuard çš„ç”Ÿå‘½å‘¨æœŸï¼Œç¡®ä¿ active_connections è®¡æ•°
/// åœ¨æµå®Œå…¨ç»“æŸåæ‰é€’å‡
fn create_sse_stream(
    response: reqwest::Response,
    ctx: StreamContext,
    initial_events: Vec<SseEvent>,
    guard: ConnectionGuard,
) -> impl Stream<Item = Result<Bytes, Infallible>> {
    // å…ˆå‘é€åˆå§‹äº‹ä»¶
    let initial_stream = stream::iter(
        initial_events
            .into_iter()
            .map(|e| Ok(Bytes::from(e.to_sse_string()))),
    );

    // ç„¶åå¤„ç† Kiro å“åº”æµï¼ŒåŒæ—¶æ¯25ç§’å‘é€ ping ä¿æ´»
    let body_stream = response.bytes_stream();

    // guard è¢«ç§»å…¥é—­åŒ…çŠ¶æ€ï¼Œéšæµä¸€èµ·å­˜æ´»
    let processing_stream = stream::unfold(
        (body_stream, ctx, EventStreamDecoder::new(), false, interval(Duration::from_secs(PING_INTERVAL_SECS)), Some(guard)),
        |(mut body_stream, mut ctx, mut decoder, finished, mut ping_interval, guard)| async move {
            if finished {
                // æµç»“æŸæ—¶ guard ä¼šè¢« dropï¼Œactive_connections é€’å‡
                drop(guard);
                return None;
            }

            // ä½¿ç”¨ select! åŒæ—¶ç­‰å¾…æ•°æ®å’Œ ping å®šæ—¶å™¨
            tokio::select! {
                // å¤„ç†æ•°æ®æµ
                chunk_result = body_stream.next() => {
                    match chunk_result {
                        Some(Ok(chunk)) => {
                            // è§£ç äº‹ä»¶
                            if let Err(e) = decoder.feed(&chunk) {
                                tracing::warn!("ç¼“å†²åŒºæº¢å‡º: {}", e);
                            }

                            let mut events = Vec::new();
                            for result in decoder.decode_iter() {
                                match result {
                                    Ok(frame) => {
                                        if let Ok(event) = Event::from_frame(frame) {
                                            let sse_events = ctx.process_kiro_event(&event);
                                            events.extend(sse_events);
                                        }
                                    }
                                    Err(e) => {
                                        tracing::warn!("è§£ç äº‹ä»¶å¤±è´¥: {}", e);
                                    }
                                }
                            }

                            // è½¬æ¢ä¸º SSE å­—èŠ‚æµ
                            let bytes: Vec<Result<Bytes, Infallible>> = events
                                .into_iter()
                                .map(|e| Ok(Bytes::from(e.to_sse_string())))
                                .collect();

                            Some((stream::iter(bytes), (body_stream, ctx, decoder, false, ping_interval, guard)))
                        }
                        Some(Err(e)) => {
                            tracing::error!("è¯»å–å“åº”æµå¤±è´¥: {}", e);
                            // å‘é€æœ€ç»ˆäº‹ä»¶å¹¶ç»“æŸ
                            let final_events = ctx.generate_final_events();
                            let bytes: Vec<Result<Bytes, Infallible>> = final_events
                                .into_iter()
                                .map(|e| Ok(Bytes::from(e.to_sse_string())))
                                .collect();
                            Some((stream::iter(bytes), (body_stream, ctx, decoder, true, ping_interval, guard)))
                        }
                        None => {
                            // æµç»“æŸï¼Œå‘é€æœ€ç»ˆäº‹ä»¶
                            let final_events = ctx.generate_final_events();
                            let bytes: Vec<Result<Bytes, Infallible>> = final_events
                                .into_iter()
                                .map(|e| Ok(Bytes::from(e.to_sse_string())))
                                .collect();
                            Some((stream::iter(bytes), (body_stream, ctx, decoder, true, ping_interval, guard)))
                        }
                    }
                }
                // å‘é€ ping ä¿æ´»
                _ = ping_interval.tick() => {
                    tracing::trace!("å‘é€ ping ä¿æ´»äº‹ä»¶");
                    let bytes: Vec<Result<Bytes, Infallible>> = vec![Ok(create_ping_sse())];
                    Some((stream::iter(bytes), (body_stream, ctx, decoder, false, ping_interval, guard)))
                }
            }
        },
    )
    .flatten();

    initial_stream.chain(processing_stream)
}

/// å¤„ç†éæµå¼è¯·æ±‚
async fn handle_non_stream_request(
    provider: std::sync::Arc<crate::kiro::provider::KiroProvider>,
    request_body: &str,
    model: &str,
    input_tokens: i32,
) -> Response {
    // è°ƒç”¨ Kiro APIï¼ˆæ”¯æŒå¤šå‡­æ®æ•…éšœè½¬ç§»ï¼‰
    let response = match provider.call_api(request_body).await {
        Ok(resp) => resp,
        Err(e) => {
            let error_msg = e.to_string();
            tracing::error!("Kiro API è°ƒç”¨å¤±è´¥: {}", error_msg);

            // æ£€æŸ¥æ˜¯å¦ä¸ºtokenè¶…é™é”™è¯¯
            if is_token_limit_error(&error_msg) {
                let context_window = super::model_config::get_context_window_size(model);
                let max_tokens = 8192; // é»˜è®¤å€¼
                return (
                    StatusCode::BAD_REQUEST,
                    Json(create_token_limit_error(input_tokens, max_tokens, context_window)),
                )
                    .into_response();
            }

            let (status, error_type) = determine_error_status(&error_msg);
            return (
                status,
                Json(ErrorResponse::new(
                    error_type,
                    format!("ä¸Šæ¸¸ API è°ƒç”¨å¤±è´¥: {}", error_msg),
                )),
            )
                .into_response();
        }
    };

    // è¯»å–å“åº”ä½“
    let body_bytes = match response.bytes().await {
        Ok(bytes) => bytes,
        Err(e) => {
            tracing::error!("è¯»å–å“åº”ä½“å¤±è´¥: {}", e);
            return (
                StatusCode::BAD_GATEWAY,
                Json(ErrorResponse::new(
                    "api_error",
                    format!("è¯»å–å“åº”å¤±è´¥: {}", e),
                )),
            )
                .into_response();
        }
    };

    // è§£æäº‹ä»¶æµ
    let mut decoder = EventStreamDecoder::new();
    if let Err(e) = decoder.feed(&body_bytes) {
        tracing::warn!("ç¼“å†²åŒºæº¢å‡º: {}", e);
    }

    let mut text_content = String::new();
    let mut tool_uses: Vec<serde_json::Value> = Vec::new();
    let mut has_tool_use = false;
    let mut stop_reason = "end_turn".to_string();
    // ä» contextUsageEvent è®¡ç®—çš„å®é™…è¾“å…¥ tokens
    let mut context_input_tokens: Option<i32> = None;

    // æ”¶é›†å·¥å…·è°ƒç”¨çš„å¢é‡ JSON
    let mut tool_json_buffers: std::collections::HashMap<String, String> =
        std::collections::HashMap::new();

    for result in decoder.decode_iter() {
        match result {
            Ok(frame) => {
                if let Ok(event) = Event::from_frame(frame) {
                    match event {
                        Event::AssistantResponse(resp) => {
                            text_content.push_str(&resp.content);
                        }
                        Event::ToolUse(tool_use) => {
                            has_tool_use = true;

                            // ç´¯ç§¯å·¥å…·çš„ JSON è¾“å…¥
                            let buffer = tool_json_buffers
                                .entry(tool_use.tool_use_id.clone())
                                .or_insert_with(String::new);
                            buffer.push_str(&tool_use.input);

                            // å¦‚æœæ˜¯å®Œæ•´çš„å·¥å…·è°ƒç”¨ï¼Œæ·»åŠ åˆ°åˆ—è¡¨
                            if tool_use.stop {
                                let input: serde_json::Value = serde_json::from_str(buffer)
                                    .unwrap_or_else(|e| {
                                        tracing::warn!(
                                            "å·¥å…·è¾“å…¥ JSON è§£æå¤±è´¥: {}, tool_use_id: {}, åŸå§‹å†…å®¹: {}",
                                            e, tool_use.tool_use_id, buffer
                                        );
                                        serde_json::json!({})
                                    });

                                tool_uses.push(json!({
                                    "type": "tool_use",
                                    "id": tool_use.tool_use_id,
                                    "name": tool_use.name,
                                    "input": input
                                }));
                            }
                        }
                        Event::ContextUsage(context_usage) => {
                            // ä»ä¸Šä¸‹æ–‡ä½¿ç”¨ç™¾åˆ†æ¯”è®¡ç®—å®é™…çš„ input_tokens
                            // è·å–æ¨¡å‹çš„context windowå¤§å°
                            let context_window_size = super::model_config::get_context_window_size(model);
                            let actual_input_tokens = (context_usage.context_usage_percentage
                                * (context_window_size as f64)
                                / 100.0)
                                as i32;
                            context_input_tokens = Some(actual_input_tokens);
                            tracing::info!(
                                "ğŸ“Š æ”¶åˆ° contextUsageEvent - ç™¾åˆ†æ¯”: {:.2}%, è®¡ç®—å¾—å‡º input_tokens: {} (ç´¯ç§¯å€¼), context_window: {}",
                                context_usage.context_usage_percentage,
                                actual_input_tokens,
                                context_window_size
                            );
                        }
                        Event::Exception { exception_type, .. } => {
                            if exception_type == "ContentLengthExceededException" {
                                stop_reason = "max_tokens".to_string();
                            }
                        }
                        _ => {}
                    }
                }
            }
            Err(e) => {
                tracing::warn!("è§£ç äº‹ä»¶å¤±è´¥: {}", e);
            }
        }
    }

    // ç¡®å®š stop_reason
    if has_tool_use && stop_reason == "end_turn" {
        stop_reason = "tool_use".to_string();
    }

    // æ„å»ºå“åº”å†…å®¹
    let mut content: Vec<serde_json::Value> = Vec::new();

    if !text_content.is_empty() {
        content.push(json!({
            "type": "text",
            "text": text_content
        }));
    }

    content.extend(tool_uses);

    // ä¼°ç®—è¾“å‡º tokens
    let output_tokens = token::estimate_output_tokens(&content);

    // ä½¿ç”¨ä» contextUsageEvent è®¡ç®—çš„ input_tokensï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ä¼°ç®—å€¼
    let final_input_tokens = context_input_tokens.unwrap_or(input_tokens);

    tracing::info!(
        "æ„å»ºéæµå¼å“åº” - input_tokens: {}, output_tokens: {}, context_input_tokens: {:?}",
        final_input_tokens,
        output_tokens,
        context_input_tokens
    );

    // æ„å»º Anthropic å“åº”
    let response_body = json!({
        "id": format!("msg_{}", Uuid::new_v4().to_string().replace('-', "")),
        "type": "message",
        "role": "assistant",
        "content": content,
        "model": model,
        "stop_reason": stop_reason,
        "stop_sequence": null,
        "usage": {
            "input_tokens": final_input_tokens,
            "output_tokens": output_tokens
        }
    });

    tracing::debug!("å“åº” usage å­—æ®µ: {{ input_tokens: {}, output_tokens: {} }}", final_input_tokens, output_tokens);

    (StatusCode::OK, Json(response_body)).into_response()
}

/// POST /v1/messages/count_tokens
///
/// è®¡ç®—æ¶ˆæ¯çš„ token æ•°é‡
pub async fn count_tokens(
    JsonExtractor(payload): JsonExtractor<CountTokensRequest>,
) -> impl IntoResponse {
    tracing::info!(
        model = %payload.model,
        message_count = %payload.messages.len(),
        "Received POST /v1/messages/count_tokens request"
    );

    let total_tokens = token::count_all_tokens(
        payload.model,
        payload.system,
        payload.messages,
        payload.tools,
    ) as i32;

    Json(CountTokensResponse {
        input_tokens: total_tokens.max(1) as i32,
    })
}
